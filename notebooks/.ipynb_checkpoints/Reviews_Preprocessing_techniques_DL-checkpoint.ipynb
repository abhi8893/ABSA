{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data for deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert file to a suited format\n",
    "\n",
    "Each review text is divided into three lines\n",
    "\n",
    "- 1st line has aspect term replaced by a placeholder token \n",
    "- 2nd line has the actual aspect term\n",
    "- 3rd line has the polarity (1, -1, 0)\n",
    "\n",
    "Finally each review text is separated by an additional newline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dl_input_file(raw_file, out_file):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dl_input_file(domain, subset, year='2014'):\n",
    "        \n",
    "    fname =f\"../data/processed/SemEval{year}/{domain}_{subset}_dl.txt\"\n",
    "    with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vec(word2idx, embed_dim):\n",
    "    \n",
    "    emb_file = f'../data/embeddings/glove.6B.{embed_dim}d.txt'\n",
    "\n",
    "    n = len(word2idx)\n",
    "    w2v = {}\n",
    "    \n",
    "    with open(emb_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in word2idx:\n",
    "                w2v[word] = np.asarray(values[1:], dtype='float32')\n",
    "                                \n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "UNIVERSAL_TAGS = {\n",
    "    \"VERB\": 1,\n",
    "    \"NOUN\": 2,\n",
    "    \"PRON\": 3,\n",
    "    \"ADJ\": 4,\n",
    "    \"ADV\": 5,\n",
    "    \"ADP\": 6,\n",
    "    \"CONJ\": 7,\n",
    "    \"DET\": 8,\n",
    "    \"NUM\": 9,\n",
    "    \"PRT\": 10,\n",
    "    \"X\": 11,\n",
    "    \".\": 12,\n",
    "}\n",
    "\n",
    "MODIFIED_TAGS = {\n",
    "    \"VERB\": 1,\n",
    "    \"NOUN\": 2,\n",
    "    \"ADJ\": 3,\n",
    "    \"ADV\": 4,\n",
    "    \"CONJ\": 5,\n",
    "    \"DET\": 6,\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pos_tag(word):\n",
    "    return nltk.pos_tag([word], tagset='universal')[0][1]\n",
    "\n",
    "def get_onehot_pos(word):\n",
    "    \n",
    "    tag = pos_tag(word)\n",
    "    arr = np.zeros(6)\n",
    "    idx = MODIFIED_TAGS.get(tag, None)\n",
    "    if idx is not None:\n",
    "        arr[idx-1] = 1\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_onehot_pos('service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "\n",
    "def create_embedding_matrix(word2idx, embed_dim=None, embed_type='glove', concat_pos_tag=False):\n",
    "    \n",
    "    if embed_type == 'glove.wiki':\n",
    "        emb_file = f'../data/embeddings/glove.wiki/glove.6B.{embed_dim}d.txt'\n",
    "        \n",
    "    elif embed_type == 'glove.twitter':\n",
    "        emb_file = f'../data/embeddings/glove.twitter/glove.twitter.27B.{embed_dim}d.txt'\n",
    "\n",
    "        \n",
    "    elif embed_type == 'amazon':\n",
    "        emb_file = '../data/embeddings/AmazonWE/sentic2vec.txt'\n",
    "        embed_dim = 300\n",
    "        \n",
    "    elif embed_type == 'google':\n",
    "        emb_file = '../data/embeddings/GoogleNews-vectors-negative300.txt'\n",
    "        embed_dim = 300\n",
    "        \n",
    "    elif embed_type in ['restaurants', 'laptops']:\n",
    "        emb_file = f'../data/embeddings/domain_embedding/{embed_type}_emb.vec.bin'\n",
    "        embed_dim = 100\n",
    "        \n",
    "        \n",
    "\n",
    "    n = len(word2idx)\n",
    "    if concat_pos_tag:\n",
    "        matrix_dim = embed_dim + 6\n",
    "    else:\n",
    "        matrix_dim = embed_dim\n",
    "    \n",
    "    embedding_matrix = np.zeros((n + 1, matrix_dim))\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    if embed_type in ['restaurants', 'laptops']:\n",
    "        domain_model = gensim.models.fasttext.load_facebook_model(emb_file)\n",
    "        \n",
    "        for word in word2idx:\n",
    "            if word in domain_model:\n",
    "                idx = word2idx[word]\n",
    "                embedding_matrix[idx][:embed_dim] = domain_model[word]\n",
    "                if concat_pos_tag:\n",
    "                    embedding_matrix[idx][embed_dim:] = get_onehot_pos(word)\n",
    "            \n",
    "            i += 1\n",
    "                \n",
    "    else:       \n",
    "        with open(emb_file, 'r') as f:\n",
    "            for line in f:\n",
    "\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "\n",
    "                if word in word2idx:\n",
    "                    idx = word2idx[word]\n",
    "                    embedding_matrix[idx][:embed_dim] = np.asarray(values[1:], dtype='float32')\n",
    "                    if concat_pos_tag:\n",
    "                        embedding_matrix[idx][embed_dim:] = get_onehot_pos(word)\n",
    "\n",
    "                    i += 1\n",
    "                \n",
    "    pct_vocab = i*100/n\n",
    "                \n",
    "    print(f'Word vectors found for {pct_vocab:.2f}% of vocabulary')\n",
    "                                \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'nt\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punct(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_tagger(review, aspect):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(review, aspect):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def split_reviews_by_aspects(dl_input_lines: list):\n",
    "    \n",
    "    reviews_raw = []\n",
    "    reviews_raw_without_aspects = []\n",
    "    reviews_left = []\n",
    "    reviews_left_with_aspects = []\n",
    "    reviews_right = []\n",
    "    reviews_right_with_aspects = []\n",
    "    postags_raw = []\n",
    "    postags_raw_left_with_aspects = []\n",
    "    postags_raw_right_with_aspects = []\n",
    "    aspects = []\n",
    "    polarities = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, len(dl_input_lines), 3):\n",
    "        review = decontracted(dl_input_lines[i])\n",
    "#         review = TextBlob(review).correct().raw\n",
    "        review_left, _, review_right = [s.lower().strip() for s in review.partition(\"$T$\")]\n",
    "        aspect = dl_input_lines[i+1].lower().strip()\n",
    "        polarity = dl_input_lines[i+2].strip()\n",
    "\n",
    "        review_raw = ' '.join([review_left, aspect, review_right])\n",
    "        review_raw = re.sub(' +', ' ', review_raw)\n",
    "        \n",
    "#         doc = nlp(review_raw)    \n",
    "#         postag_raw = [token.tag_ for token in doc]\n",
    "#         postags_raw.append(postag_raw)\n",
    "        \n",
    "        \n",
    "        reviews_raw.append(review_raw)\n",
    "        reviews_raw_without_aspects.append(review_left + \" \" + review_right)\n",
    "        reviews_left.append(review_left)\n",
    "        reviews_left_with_aspects.append(review_left + \" \" + aspect)\n",
    "        reviews_right.append(review_right)\n",
    "        reviews_right_with_aspects.append(aspect + \" \" + review_right)\n",
    "        aspects.append(aspect)\n",
    "        polarities.append(int(polarity))\n",
    "        \n",
    "        \n",
    "    res = {\n",
    "        'reviews_raw': reviews_raw,\n",
    "        'reviews_raw_without_aspects': reviews_raw_without_aspects,\n",
    "        'reviews_left': reviews_left,\n",
    "        'reviews_left_with_aspects': reviews_left_with_aspects,\n",
    "        'reviews_right': reviews_right,\n",
    "        'reviews_right_with_aspects': reviews_right_with_aspects,\n",
    "        'aspects': aspects,\n",
    "        'polarities': polarities,\n",
    "        'postags_raw': postags_raw\n",
    "    }\n",
    "        \n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_data(texts, maxlen, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_dl(domain='restaurants', subset='train', embed_dim=300, embed_type='glove', \n",
    "                        max_input_len=70, max_aspect_len=5, num_classes=3, tokenizer=None, concat_pos_tag=False):\n",
    "    \n",
    "    \n",
    "    if domain == 'both':\n",
    "        lines_rest = read_dl_input_file('restaurants', subset)\n",
    "        lines_lap = read_dl_input_file('laptops', subset)\n",
    "        lines = lines_rest + lines_lap\n",
    "        \n",
    "    else:\n",
    "        # Read the lines from the pre-formatted dl input file\n",
    "        lines = read_dl_input_file(domain, subset)\n",
    "    \n",
    "    # now obtain the splitted reviews on the left and right side of the aspect\n",
    "    spltd = split_reviews_by_aspects(lines)\n",
    "    polarities = spltd.pop('polarities')\n",
    "    postags_raw = spltd.pop('postags_raw')\n",
    "    \n",
    "    # Tokenize \n",
    "    if subset == 'test':\n",
    "        if tokenizer is None:\n",
    "            raise ValueError('Provide a tokenizer fitted on the train data!')\n",
    "        if max_input_len is None:\n",
    "            raise ValueError('Provide a maximum input length for padding the input sequence!')\n",
    "        if max_aspect_len is None:\n",
    "            raise ValueError('Provide a maximum aspect length for padding the aspect terms!')\n",
    "            \n",
    "    elif subset == 'train':\n",
    "        tokenizer = Tokenizer(lower=False)\n",
    "        tokenizer.fit_on_texts(spltd['reviews_raw'])\n",
    "        \n",
    "        \n",
    "    word2idx = tokenizer.word_index\n",
    "    \n",
    "    # Create sequence padded data of indices\n",
    "    res = {}\n",
    "    \n",
    "    for k, v in spltd.items():\n",
    "        if k == 'aspects':\n",
    "            maxlen = max_aspect_len\n",
    "        else:\n",
    "            maxlen = max_input_len\n",
    "            \n",
    "        res[f'{k}_idx'] = create_sequence_data(v, maxlen, tokenizer)\n",
    "        \n",
    "    # one hot encode polarities\n",
    "    res['polarity_ohe'] = to_categorical(polarities, num_classes)\n",
    "    res['postags_raw'] = postags_raw\n",
    "        \n",
    "    if subset == 'test':\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    if type(embed_type) is list:\n",
    "        embedding_matrix = []\n",
    "        for emb_type in embed_type:\n",
    "            embedding_matrix.append(create_embedding_matrix(word2idx, embed_dim, emb_type, concat_pos_tag))\n",
    "            concat_pos_tag = False\n",
    "            \n",
    "        embedding_matrix = np.hstack(embedding_matrix)\n",
    "        \n",
    "    else:\n",
    "        embedding_matrix = create_embedding_matrix(word2idx, embed_dim, embed_type, concat_pos_tag)\n",
    "                                  \n",
    "    res['embedding_matrix'] = embedding_matrix\n",
    "    res['tokenizer'] = tokenizer\n",
    "\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/miniconda3/envs/absa/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/abhi/miniconda3/envs/absa/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors found for 100.00% of vocabulary\n"
     ]
    }
   ],
   "source": [
    "res = prepare_data_for_dl(concat_pos_tag=True, embed_type='restaurants')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
