{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "from utils import load_raw_file, remove_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    words = [word for word in tokens if word.isalnum()]\n",
    "    return words\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    df['text'] = df['text'].apply(tokenize)\n",
    "    df['term'] = df['term'].str.lower()\n",
    "    df['term'] = df['term'].apply(tokenize)\n",
    "    df['term'] = df['term'].apply(lambda x:\" \".join(x))\n",
    "    return df\n",
    "\n",
    "polarity_label = {\n",
    "    'positive': 1,\n",
    "    'negative': -1,\n",
    "    'neutral': 0\n",
    "}\n",
    "\n",
    "\n",
    "def create_vocab(df):\n",
    "    text_w2id = {'<pad>': 0}\n",
    "    term_w2id = {}\n",
    "    for words in df['text']:\n",
    "        for word in words:\n",
    "            if word not in text_w2id:\n",
    "                text_w2id[word] = len(text_w2id)\n",
    "\n",
    "    for words in df['term']:\n",
    "        if words not in term_w2id:\n",
    "            term_w2id[words] = len(term_w2id)\n",
    "    return text_w2id, term_w2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get the relative position of the token w.r.t. aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_token_pos(row):\n",
    "    index = []\n",
    "    s_len = len(row['text'])-1\n",
    "    p = row['text'].copy()\n",
    "    aspects = row['term'].split(' ')\n",
    "    for aspect in aspects:\n",
    "        try:\n",
    "            if len(aspects)-1 > aspects.index(aspect):\n",
    "                a_i = [i for i,val in enumerate(row['text']) if val==aspect]\n",
    "                try:\n",
    "                    for a_id in a_i:\n",
    "                        if row['text'][a_id+1] != aspects[aspects.index(aspect)+1]:\n",
    "                            a_i.remove(a_id)\n",
    "                except:\n",
    "                    pass\n",
    "                index.extend(a_i[0])\n",
    "            else:\n",
    "                index.append(row['text'].index(aspect))\n",
    "            p[row['text'].index(aspect)] = s_len\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        for i in range(index[0]):\n",
    "            p[i] = s_len - index[0] + i\n",
    "        v = s_len\n",
    "        for i in range(index[len(index)-1],len(p)):\n",
    "            if i == index[len(index)-1]:\n",
    "                p[i] = v\n",
    "            else:    \n",
    "                p[i] = v - 1\n",
    "                v = v-1\n",
    "            \n",
    "    except Exception as e: \n",
    "        p = [0 for i in row['text']]\n",
    "    text_pos_data.append(p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(row):\n",
    "    global max_tokens\n",
    "    global text_w2id\n",
    "    global term_word2idx\n",
    "    \n",
    "    text_ids = [text_w2id[idx] for idx in row['text']]\n",
    "    text_data.append(text_ids)\n",
    "    \n",
    "    n = len(text_ids)\n",
    "\n",
    "    if n > max_tokens:\n",
    "        max_tokens = n\n",
    "        \n",
    "    term_ids = [term_w2id[row['term']]]\n",
    "    term_data.append(term_ids)\n",
    "    \n",
    "    \n",
    "    polarity.append(polarity_label[row['polarity']])\n",
    "    pos_ids = get_relative_token_pos(row)\n",
    "    \n",
    "    prepared_row = [text_ids, pos_ids, term_ids]\n",
    "    prepared_rows.append(prepared_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(df, text2id, term2id):\n",
    "    global text_w2id\n",
    "    global term_w2id\n",
    "    text_w2id = text2id\n",
    "    term_w2id = term2id\n",
    "\n",
    "    df.apply(prepare_data,axis = 1)\n",
    "    return text_data, text_pos_data, term_data, polarity, max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some global variables \n",
    "\n",
    "These will be used to featurize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_w2id = {}\n",
    "term_w2id = {}\n",
    "text_data = []\n",
    "text_pos_data = []\n",
    "term_data = []\n",
    "polarity = []\n",
    "\n",
    "prepared_rows = []\n",
    "max_tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = load_raw_file('restaurants', 'train')\n",
    "df_train = remove_polarity('conflict', df_train)\n",
    "\n",
    "\n",
    "df_test = load_raw_file('restaurants', 'test')\n",
    "df_test = remove_polarity('conflict', df_test)\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "\n",
    "test_start_id = df_train.shape[0]\n",
    "\n",
    "df_prep = preprocess(df)\n",
    "text_w2id, term_w2id = create_vocab(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(X, y):\n",
    "    X_train, y_train = X[:test_start_id], y[:test_start_id]\n",
    "    X_test, y_test = X[test_start_id:], y[test_start_id:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data, text_pos_data, term_data, polarity, max_tokens = generate_data(df_prep, text_w2id, term_w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_data(text_data, text_pos_data, term_data, polarity, max_tokens):\n",
    "    \n",
    "    row_arr = []\n",
    "    for i in range(len(text_data)):\n",
    "        x1, x2, x3 = [], [], []\n",
    "\n",
    "        x1.extend(text_data[i])\n",
    "        if len(x1) < max_tokens:\n",
    "            for _ in range(max_tokens - len(x1)):\n",
    "                x1.append(0)\n",
    "\n",
    "        x2.extend(text_pos_data[i])\n",
    "        if len(x2) < max_tokens:\n",
    "            for _ in range(max_tokens - len(x2)):\n",
    "                x2.append(0)\n",
    "\n",
    "        x3.extend(term_data[i])\n",
    "        for j in range(len(x1) - 1):\n",
    "            x3.append(x3[0])\n",
    "        for k in range(max_tokens - len(x3)):\n",
    "            x3.append(0)\n",
    "\n",
    "        row_arr.append([x1, x2, x3])\n",
    "\n",
    "    row_arr = np.array(row_arr)\n",
    "    nobs, n1, n2 = row_arr.shape\n",
    "    X = row_arr.reshape((nobs, n1*n2))\n",
    "    y = np.array(polarity)\n",
    "    \n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = featurize_data(text_data, text_pos_data, term_data, polarity, max_tokens)\n",
    "X_train, y_train, X_test, y_test = get_train_test_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.5)\n",
    "clf.fit(X_resampled, y_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
